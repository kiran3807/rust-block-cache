# rust-block-cache
Take home project to simulate a HTTP block-cache-service used filter out HTTP clients, with the original block file generated by another external service

The module is exposed as a library crate. 
`cargo run` to test and play around with the lib.

The problem statement is present within `problem_statement.md` for your reference.


## Assumptions :

### Shape of the final solution:

I interpret the following line from the problem statement:

 *Your task is to implement a block cache in Rust with the following public
signature:* 

as to mean, that I am to expose the block cache as a **library** for external use, For instance say within a third party web service.

Hence I make no assumptions regarding the environment and runtimes within which the library may be used.

### Block Heirarchy Logic :

* In case of IP-address and user agent combination occouring more than once with different fine grained blocks, the latest entry within the block file wins.

* Similarly in case of IP address - fine grained block combination that occours more than once the latest entry wins

* You can never have an entry where there is only user-agent, or just user-agent and fine grained block. The broadest block that can be applied exists only on IP level.

* In case of a *particular IP address and user agent combination that has not been explicitly mentioned in the blockfile*, we should return 0. In otherwords, when there is no general fine block associated with an IP, any specific combination IP and user agent should be allowed to pass through as long as there is no explicit fine grained block associated with that combination.

### Data access patterns and Memory requirements :

* **High reads** for ```get_blocks``` with short payloads  : 5k reads per second, 20k peak
* **Low volume** of reads for the reading blockfile with a significant larger payload every 5 seconds.

**Number of reads** : This based on the fact that typical enterprise points of presence often handle 100k requests /second or more however it is distributed over multiple services. Hence 5k as lower limit and 20k as upper limit.

**Size of the blockfile** :

Each line consists of atmost:
* IP Address,
* Fine grained block
* User-Agent

a UTF-8 character is typically 4 bytes at max, 

* IP address, consisting of 15 characters = 15*4 = 60bytes
* The largest user agent, as seen in the sample_block_file.txt, is about 520 bytes and the smallest is 16 bytes, thus average size would be: 268 bytes
* fine grained block is a single digit hence 4 bytes
* 3 commas which amount 3*4 = 12 bytes

Total = 344 bytes or 350 bytes rounded per line of the block-file

Which implies in memory at worst it would not exceed 700MB where I assume the specifics of the data structure storing the data might cause it occupy pessimistically twice the volume as the original 350 MB(350 * 1000000). 

While it is within the capabilities of modern devices, however certain edge devices with low memory provisions(less than 2GB) may struggle with thrashing as OS triggers page swaps



## Solution : 

The basic idea is that given the constraints pertaining to *the volume of data* and *low latency reads* an in-memory HashMap continues to be the fastest ways `get_block` function calls can be serviced. 

Expecially in context of the fact that The SipHash function used by Rust in it's HashMap(also by IndexHash crate from which I import IndexSet) implementation even with million entries, it can be demonstrated to have a low probability of collision.

From the Birthday problem, used to calculate probability of collisions:

![equation](https://latex.codecogs.com/svg.latex?P(\text{collision})%20\approx%201%20-%20e^{-\frac{n(n-1)}{2M}})


M=64 since SipHash is 64 bit hash and for n = 1000000, we get 2.7×10−6 as the probability of collisions.

Secondly to ensure low latency of reads we have to for atleast a certain period of time maintain two data-sets within memory, the older one that is being used to service requests while the newer one is being constructed corresponding to the updated block-file( which is updated every 5 seconds) before it can be completely replace the older data.

Which only underlines the need reduce memory foot print of the data-structures holding the block-file data, not only because at some point memory holds data associated with two blockfiles, but constructing the data-structures to hold the new block-file data, is time consuming and should not be consuming the better part of the 5 second interval after which the block-files is updated.


### Data patterns :

Within the examples for the block file provided within the problem statement, we observe that:

* You can have the same IP address repeated in context of different user agents
* The possibility of the same user agent being repeated in context of different IP addresses.


### Optimization - 1 : 

Therefore I maintain two IndexSets(https://crates.io/crates/indexmap ) one corresponding to IP address and the other corresponding to User agent.

I then construct a tuple key consisting of the unique indexes each measuring 4 bytes each of the IP addresses and User agents in the above IndexSets and map it to the fine grained block associated with it. So HashMap where the actual mapping happens has very low memory footprint.

At best the worst case memory complexity is when we have utterly unique IP address and user agent combinations in every line of the block-file and therefore the memory complexity is marginally larger than what I would have had if I used a simple HashMap. 

But this comes with the advantage of a reduced memory foot print and performance for the average case, where we assume there will be lot of repitition within IP addresses and user agents.

It also allows for minor speed improvements as within the HashMap where we store the mapping between ips, user agents and fine grained blocks, the keys are now much small in size and hence can be hashed faster.

### Optimization - 2 : 

However in certain cases we cannot avoid the worst case memory complexity and stressing the memory of the edge services. So to avoid thrashing and to *further boost read latency in general*, we place a LRU cache within the get_block functionality that gaurds the access to the actual logic.

## Going further :

### Bloom Filters : 

In general if we assume 80-60 percent of requests made to a given node are not malicious, we can ensure our logic is only restricted to the greater subset of malicious requests by the means of a bloom filter.

Bloom filter is capable of, with greatly reduced memory footprint and high speed figuring out if a given IP address has any sort of block associated with it.

We simply have to query if the incoming IP address is not present within the ip addresses stored by the bloom filter. If the answer is negative that means there is neither a IP level fine block or IP-user agent-fine block entry within the block-file.

Hence we can return 0 there itself, allowing the IP to pass.


### Btree Maps :

As mentioned previously for a certain time period the memory holds data associated with two different block-files, straining the memory of low spec edge devices. HashMap performance often degrades in such contexts on account of thrashing due to the page swaps the initiated by the OS.

This is due to lack of cache locality in HashMaps. In that case we can use BtreeMaps, where we sacrifice o(1) lookup for log(n) lookup in return for much better cache locality, preventing thrashing caused by OS page swaps.


